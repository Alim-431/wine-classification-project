# -*- coding: utf-8 -*-
"""Projet_Inf1823.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ncjXT5qk9JZEH1h6uljkXeUh2kt6dtn
"""

import pandas as pd
from sklearn.datasets import load_wine
import matplotlib.pyplot as plt
import seaborn as sns

"""**Partie A**

Nous avons calculé la matrice de corrélation du jeu de données Wine à l’aide du coefficient de Pearson.
Nous avons identifié une forte corrélation (r = 0.86) entre les variables flavanoids et total_phenols.
Cela indique que ces deux caractéristiques chimiques évoluent dans le même sens, les vins riches en flavonoïdes tendent également à contenir davantage de phénols totaux.


Le nuage de points montre une relation linéaire positive entre les variables flavanoids et total_phenols. Le coefficient de corrélation de Pearson r=0.86 indique une forte association linéaire : les vins riches en flavonoïdes tendent également à contenir davantage de phénols totaux. Cette relation est cohérente avec la composition chimique du vin, puisque les flavonoïdes font partie des phénols.


Nous avons choisi les variables flavanoids et total_phenols car, après avoir calculé la matrice de corrélation du jeu de données Wine, il est apparu que ces deux mesures présentaient l’une des corrélations les plus élevées (r ≈ 0.86).
Ce choix est justifié scientifiquement : les flavonoïdes font partie des phénols présents dans le vin, donc il est logique qu’une augmentation de l’un soit associée à une augmentation de l’autre.
En d’autres termes, la relation chimique entre ces deux composés explique naturellement une corrélation aussi forte.
"""

print("Partie A")

#creation du data frame
wine = load_wine()
data = pd.DataFrame(wine.data, columns=wine.feature_names)
target = wine.target
print(data)
print(target)

matrice_correlation = data.corr(numeric_only=True)
#print(matrice_correlation)
matrice_paires = matrice_correlation.unstack()
matrice_paires  = matrice_paires [matrice_paires  < 1]
Resultats = matrice_paires .sort_values(ascending=False)

# Afficher les 10 corrélations les plus fortes
print("\nLes 10 paires de variables les plus corrélées :")
print(Resultats.head(10))
print("\n")

#Representation graphiqye
print("Représentatio graphique de la relation")
x = data['flavanoids']
y = data['total_phenols']
r = x.corr(y)
print("Coefficient de corrélation de Pearson : r = ", r)
plt.scatter(x, y, color='blue')
plt.title("Corrélation entre flavanoids et total_phenols")
plt.xlabel("Flavanoids")
plt.ylabel("Phenols")
plt.grid(True)
plt.show()

"""**Partie B-1**

Pour prédire la classe des nouvelles observations, nous avons utilisé l’algorithme KNN.
Celui-ci compare les nouvelles données aux vins existants en calculant la distance entre leurs caractéristiques.
Les K vins les plus proches (ici K = 5) déterminent la classe probable de la nouvelle observation.
Cette méthode est adaptée au jeu de données Wine car il contient uniquement des variables numériques et un nombre raisonnable d’échantillons, ce qui rend le calcul des distances fiable et rapide.
Après normalisation des données pour neutraliser les différences d’échelle, le modèle fournit une classification cohérente et stable.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

print("Partie B-1")
X = wine.data
y = wine.target

#  Diviser le dataset en train et test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y)

#  Normalisation
scaler = StandardScaler().fit(X_train)
X_train_s = scaler.transform(X_train)

# Modèle KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_s, y_train)

#  Nouvelle observation
new_d = np.array([13, 2, 2, 20, 99, 2, 2, 0.4, 2, 5, 1, 2.5, 500]).reshape(1, -1)

# Normalisation de new_d
new_d_s = scaler.transform(new_d)


prediction = knn.predict(new_d_s)[0]
print("Classe prédite :", prediction)
print("Nom de la classe :", wine.target_names[prediction])

"""**Partie B-2**

Pour traiter la nouvelle observation comportant des valeurs manquantes, nous avons utilisé une imputation par la moyenne.
Cette méthode consiste à remplacer les données manquantes par la moyenne de la colonne correspondante dans le jeu d’entraînement.
Elle est appropriée ici car les valeurs imputées restent cohérentes avec la distribution des autres vins et ne créent pas de distorsion majeure dans les données.
Une fois l’observation complétée, nous avons appliqué le même modèle KNN que dans la partie précédente.
La classification obtenue est stable et crédible, ce qui montre que l’imputation utilisée est raisonnable pour ce jeu de données.
"""

from sklearn.impute import SimpleImputer, KNNImputer

print("Partie B-2")

# Observation manquante
new_d1 = np.array([np.nan, np.nan, 3, 15, 80, 3, 1, 0.3, 2, 5, 1, 2.5, 500]).reshape(1, -1)

# imputation par la moyenne
imputation_moy = SimpleImputer(strategy='mean').fit(X_train)
new_d1_moyenne = imputation_moy.transform(new_d1)

# MÉTHODE 2 : imputation KNN
imputation_knn = KNNImputer(n_neighbors=5).fit(X_train)
new_d1_knn = imputation_knn.transform(new_d1)


pred_moyenne = knn.predict(scaler.transform(new_d1_moyenne))[0]
pred_knn = knn.predict(scaler.transform(new_d1_knn))[0]

print("Imputation par la moyenne")
print(new_d1_moyenne)
print("Classe :", wine.target_names[pred_moyenne])

print("\n=== Imputation par KNN ===")
print(new_d1_knn)
print("Classe :", wine.target_names[pred_knn])

"""**`Partie C : Réduction de dimension et impact sur la classification`**

**C1 — Comparaison des techniques de réduction
Nous avons comparé deux méthodes principales :PCA et LDA.**

- PCA (Analyse en Composantes Principales) est une technique non supervisée qui projette les données sur les axes expliquant le plus de variance. Elle conserve l’information globale mais ne tient pas compte des classes.

- LDA (Linear Discriminant Analysis) est une technique supervisée qui cherche à maximiser la séparation entre les classes. Étant donné que le dataset Wine comporte 3 classes, LDA permet une réduction maximum à 2 dimensions.
"""

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
print("Partie C-1")

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modèle KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

baseline_score = knn.score(X_test_scaled, y_test)
print("Précision SANS réduction :", baseline_score)

#  PCA
print("\nPCA  ")

for n in [2, 3, 5, 7, 10]:
    pca = PCA(n_components=n)
    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)

    knn.fit(X_train_pca, y_train)
    score = knn.score(X_test_pca, y_test)

    print(f"PCA avec {n} composantes → précision = {score:.4f}")

# LDA
print("\n LDA  ")
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_scaled, y_train)
X_test_lda = lda.transform(X_test_scaled)

knn.fit(X_train_lda, y_train)
lda_score = knn.score(X_test_lda, y_test)

print("LDA avec 2 composantes → précision =", lda_score)

"""**C2 — Impact de la réduction sur la performance de classification**

Nous avons appliqué la classification KNN avant et après réduction de dimensionnalité.
Sans réduction, la précision du modèle est d’environ 95–99%.
- Avec PCA, la précision dépend du nombre de composantes :
2 composantes : baisse de précision (≈ 85–93%)
3 à 5 composantes : précision proche du modèle original
7 composantes : quasi aucune perte
- Avec LDA, la réduction à 2 dimensions offre les meilleures performances (≈ 96–100%), supérieures à PCA lorsque le nombre de composantes est faible.

**Conclusion**
LDA est la méthode la plus efficace pour maintenir la performance de classification, car elle préserve l’information discriminante.
PCA devient compétitif lorsque le nombre de composantes est supérieur ou égal à 5.

**Partie D : Robustesse des méthodes de classification**

Pour vérifier que nos résultats ne dépendent pas d’un seul découpage du dataset, nous avons utilisé la validation croisée K-fold, qui est la méthode la plus répandue pour évaluer la robustesse d’un modèle.
Le principe de cette méthode est de diviser les données en K blocs, d’entraîner le modèle sur K–1 blocs, puis de tester sur le bloc restant.
Cette opération est répétée K fois avec un bloc différent, puis les précisions obtenues sont moyennées.
"""

from sklearn.model_selection import cross_val_score
print("Partie D")


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
knn = KNeighborsClassifier(n_neighbors=5)
scores_base = cross_val_score(knn, X_scaled, y, cv=5)
print("KNN sans réduction → moyenne :", scores_base.mean(),
      " / variance :", scores_base.var())


#  PCA
for n in [2, 3, 5, 7]:
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)

    scores_pca = cross_val_score(knn, X_pca, y, cv=5)
    print(f"PCA {n} dimensions → moyenne : {scores_pca.mean()} / variance : {scores_pca.var()}")


# LDA

lda = LDA(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)
scores_lda = cross_val_score(knn, X_lda, y, cv=5)
print("LDA 2D → moyenne :", scores_lda.mean(),
      " / variance :", scores_lda.var())